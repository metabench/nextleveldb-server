Thinking of subclassing the server.
It would have other specific functionality.
Could have a collector-server?

A Server GUI would also be useful.
Could have something just saying that it's running. Say what the path is. Say what the IO/s is.

A server with its specific model built in.
When starting up, will make sure it has that model.
Can have that built into the server component.

Soon will need to make an authenticated DB system.
Would use a very simple authentication systm to start with, with usernames and passwords in a config file.
Could have separate authentication db or service
Could have authentication within this db.
Could restrict by IP address.

Definitely need authentication when connecting
Could have authorisation for tasks
Could get a session token when the connection is opened.

Client and maybe ll client commands such as list users

Including authentication and authorisation tables into the model.
Could have another file or function to do that.

Need a somewhat dynamic way of optionally including specific functionality in the initial model. Feature flags could do it.

A separate authorisation and authentication database / service could be useful too.
Could also provide a&a services to the main db. A number of DBs on the network could refer to that single A&A DB for credential checking.
To start with, that system could just have a config file of users, with their permissions and roles.

Could allow read table access to any user (could say default user), for some specific tables.
The authorisation service could track requests to see how many had been done recently, to see if limits have been exceeded, or to charge for request authorisation and fulfilment.


// NextLevelDB_Auth_Service
//  This would possibly have its own NextLevelDB a bit later on. Maybe NextLevelDB_Auth_DB.
//  The Auth Service could do things such as give out an access token which will last a few days in response to the correct username and password, could be loaded from a config file.
//   Or could check against password hashes.


// The server could be set up to use an auth service. The auth service, or net-auth-service would run on a network port.
//  Would have a binary interface, and there would be the same command space as nextleveldb-server. The server could use some of those auth funtions itself.

// For the moment, don't need to make the auth-db.
//  There will be a limited number of users granted access for the moment.
//  The auth service could work to prevent DOS attacks by authorising normal requests from unknown ip addresses, but keeping track of the rate.



Just
Auth_Service
  That will be used by the NextLevelDB.

AA_Service
// Authentication and Authorization

// Could check for permission to do a variety of tasks against the auth service

// Check for a password match, then grant an Authentication token.
// A1 = authentication, A2 = authorisation

// 


// The server could be given an authorisation service address.
//  Maybe it could communicate with websockets.
//  Should probably be a binary message system, like with NextLevelDB Server and Client.
//  Will use Binary_Encoding to encode the sent messages.









NextLevelDB_Auth_Service












For the moment, getting this running with stability is a priority.



After having it running with stability, replication, and possibly getting records in an efficient data structure is next.
Be able to run another db process, and have it stream all of the data from a source database.

  One time sync vs connected sync.

    1) Be able to copy by table first?
    2)  Would mean synchronising the cores.
          Various pitfalls, need to avoid them.
          With a blank database, can copy from another database. Could copy the core, and then copy the various tables over.

    2) Direct copy of records - where appropriate.
        Could do this when table ids and pks match.
    
Want to be able to have a local copy of the DB that has got its records from a remote online DB.

A db with the command copy from remote would make sense.

Could be a binary command.

Would get the model rows from the remote db, then copy over each of the tables separately.

Saving the DB replication settings in the DB?
Seems like a DB properties table would be useful
Even a remote db table?

May be worth keeping the DB itself aware of where it will sync to / from.
That way a db could be told which other DBs to sync from.

Could have a 'peers' table.
Could subscribe to the updates to such a connected database, and also catch up on the backlog.

Catching up on backlog seems a bit tricky though, making blocks of data seems like a good way for this, even a simple blockchain.
May require a separate way of storing the files, not keen on that.

Maybe copying the entire database will be the way to do this.

Counting records in ranges, and comparing keys.

Could get the hash of key ranges, and download them if they don't match.

A hash of all of the keys in a table would help to check that all of them have been copied OK.

Want it so that it can be reliably started up, and known to have a full copy of the data once the process has been completed.
May be best done with a db snapshot.

Stream DB snapshot

Then syncing to the up-to-date records could at first also be done with a full snapshot.
Could have a system to check for / retrieve the earlier records, but this would vary depending on table encoding.


25/02/18 - Improving the paging and data encoding of messages. Making it so that it's faster / requires less code to decode messages.
Essentially, making a smaller API.



















    




Also enabling more exchanges' data. Want to get data for a number more exchanges.
The collector-base will be useful for this. Could start with quite a lot declared already.

